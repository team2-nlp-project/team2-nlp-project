{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078ec5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8a2d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"h3 lh-condensed\">\n",
       "<a data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"TRENDING_REPOSITORIES_PAGE\",\"click_target\":\"REPOSITORY\",\"click_visual_representation\":\"REPOSITORY_NAME_HEADING\",\"actor_id\":null,\"record_id\":39840932,\"originating_url\":\"https://github.com/trending\",\"user_id\":null}}' data-hydro-click-hmac=\"b5c21c00a03199cc476d548acb97ab258f58b11b86fc31b42a24574f04824f67\" data-view-component=\"true\" href=\"/google/googletest\">\n",
       "<svg aria-hidden=\"true\" class=\"octicon octicon-repo mr-1 color-fg-muted\" data-view-component=\"true\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\">\n",
       "<path d=\"M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z\" fill-rule=\"evenodd\"></path>\n",
       "</svg>\n",
       "<span class=\"text-normal\" data-view-component=\"true\">\n",
       "        google /\n",
       "</span>\n",
       "      googletest\n",
       "</a>\n",
       "</h1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/trending' \n",
    "response = get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "headings = soup.select('h1.h3.lh-condensed')\n",
    "headings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9f7850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9549581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google/googletest',\n",
       " 'projectdiscovery/nuclei-templates',\n",
       " 'digitalocean/nginxconfig.io',\n",
       " 'flutter/flutter',\n",
       " 'PaddlePaddle/PaddleOCR',\n",
       " 'flutter/pinball',\n",
       " 'wolfogre/go-pprof-practice',\n",
       " 'supabase/supabase',\n",
       " 'felipefialho/frontend-challenges',\n",
       " 'flutter/samples',\n",
       " 'alibaba/fastjson2',\n",
       " 'florinpop17/app-ideas',\n",
       " 'charmbracelet/bubbletea',\n",
       " 'huggingface/transformers',\n",
       " 'databricks-academy/data-engineering-with-databricks',\n",
       " 'hectorqin/reader',\n",
       " 'Azure/azure-rest-api-specs',\n",
       " 'terra-money/core',\n",
       " 'saltstack/salt',\n",
       " 'PKUFlyingPig/cs-self-learning',\n",
       " 'actions/virtual-environments',\n",
       " 'jojoldu/junior-recruit-scheduler',\n",
       " 'dotnet/aspnetcore',\n",
       " 'danielgindi/Charts',\n",
       " 'microsoft/unilm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[h.select_one('a').attrs['href'][1:] for h in soup.select('h1.h3.lh-condensed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#### Pulled from class will need to add values\n",
    "\n",
    "\"\"\"\n",
    "A module for obtaining repo readme and language data from the github API.\n",
    "Before using this module, read through it, and follow the instructions marked\n",
    "TODO.\n",
    "After doing so, run it like this:\n",
    "    python acquire.py\n",
    "To create the `data.json` file that contains the data.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "\n",
    "from env import github_token, github_username\n",
    "\n",
    "# TODO: Make a github personal access token.\n",
    "#     1. Go here and generate a personal access token https://github.com/settings/tokens\n",
    "#        You do _not_ need select any scopes, i.e. leave all the checkboxes unchecked\n",
    "#     2. Save it in your env.py file under the variable `github_token`\n",
    "# TODO: Add your github username to your env.py file under the variable `github_username`\n",
    "# TODO: Add more repositories to the `REPOS` list below.\n",
    "\n",
    "REPOS = [\n",
    " \n",
    " 'topkecleon/telegram-bot-bash',\n",
    " 'evil-mad/EggBot',\n",
    " 'telegram-bot-rb/telegram-bot',\n",
    " 'alfficcadenti/splinterlands-bot',\n",
    " 'nodeWechat/wechat4u',\n",
    " 'tucnak/telebot',\n",
    " 'CarlGroth/Carl-Bot',\n",
    " 'grapeot/WechatForwardBot',\n",
    " 'googleworkspace/hangouts-chat-samples',\n",
    " 'GreyWolfDev/Werewolf',\n",
    " 'abdelhai/awesome-bots',\n",
    " 'mdgspace/bot',\n",
    " 'Sank6/Discord-Bot-List',\n",
    " 'samc621/SneakerBot',\n",
    " 'boto/boto3-sample',\n",
    " 'huseinzol05/Stock-Prediction-Models',\n",
    " 'SAPConversationalAI/Webchat',\n",
    " 'soumyadityac/youtube-viewer',\n",
    " 'CodeWithJoe2020/pancakeswapBot',\n",
    " 'Merubokkusu/discord-spam-bots',\n",
    " 'ZeroDiscord/EconomyBot',\n",
    " 'scrapinghub/slackbot',\n",
    " 'baidu/boteye',\n",
    " 'kcloze/swoole-bot',\n",
    " 'hyperchessbot/hyperbot',\n",
    " 'yangyuan/hearthrock',\n",
    "]\n",
    "\n",
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "    raise Exception(\n",
    "        \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "    )\n",
    "\n",
    "\n",
    "def github_api_request(url: str) -> Union[List, Dict]:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Error response from github api! status code: {response.status_code}, \"\n",
    "            f\"response: {json.dumps(response_data)}\"\n",
    "        )\n",
    "    return response_data\n",
    "\n",
    "\n",
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    repo_info = github_api_request(url)\n",
    "    if type(repo_info) is dict:\n",
    "        repo_info = cast(Dict, repo_info)\n",
    "        if \"language\" not in repo_info:\n",
    "            raise Exception(\n",
    "                \"'language' key not round in response\\n{}\".format(json.dumps(repo_info))\n",
    "            )\n",
    "        return repo_info[\"language\"]\n",
    "    raise Exception(\n",
    "        f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "    url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "    contents = github_api_request(url)\n",
    "    if type(contents) is list:\n",
    "        contents = cast(List, contents)\n",
    "        return contents\n",
    "    raise Exception(\n",
    "        f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a response from the github api that lists the files in a repo and\n",
    "    returns the url that can be used to download the repo's README file.\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        if file[\"name\"].lower().startswith(\"readme\"):\n",
    "            return file[\"download_url\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def process_repo(repo: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "    dictionary with the language of the repo and the readme contents.\n",
    "    \"\"\"\n",
    "    contents = get_repo_contents(repo)\n",
    "    readme_download_url = get_readme_download_url(contents)\n",
    "    if readme_download_url == \"\":\n",
    "        readme_contents = \"\"\n",
    "    else:\n",
    "        readme_contents = requests.get(readme_download_url).text\n",
    "    return {\n",
    "        \"repo\": repo,\n",
    "        \"language\": get_repo_language(repo),\n",
    "        \"readme_contents\": readme_contents,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_github_data():\n",
    "    \"\"\"\n",
    "    This function checks for a local csv file of raw data. If a local csv exists it reads it \n",
    "    to a df.\n",
    "    \n",
    "    If no local csv of raw data exists, it loops through all of the repos in the REPOS list above and\n",
    "    and runs the process functions to scrape the repo name, language, and readme\n",
    "    contents. It then writes the resulting dictionary of lists to a df, returning\n",
    "    a dataframe of raw data. It then caches the raw data to a local csv file.\n",
    "    \"\"\"\n",
    "    if os.path.isfile('raw_data.csv'):\n",
    "        df = pd.read_csv('raw_data.csv')\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame([process_repo(repo) for repo in REPOS])\n",
    "        df.to_csv('raw_data.csv')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_github_data()\n",
    "    json.dump(data, open(\"data.json\", \"w\"), indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
