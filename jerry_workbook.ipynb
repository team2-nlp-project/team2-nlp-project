{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b263057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# A module for obtaining repo readme and language data from the github API.\n",
    "\n",
    "# Before using this module, read through it, and follow the instructions marked\n",
    "# TODO.\n",
    "\n",
    "# After doing so, run it like this:\n",
    "\n",
    "#     python acquire.py\n",
    "\n",
    "# To create the `data.json` file that contains the data.\n",
    "# \"\"\"\n",
    "# import os\n",
    "# import json\n",
    "# from typing import Dict, List, Optional, Union, cast\n",
    "# import requests\n",
    "\n",
    "# from env import github_token, github_username\n",
    "\n",
    "# # TODO: Make a github personal access token.\n",
    "# #     1. Go here and generate a personal access token: https://github.com/settings/tokens\n",
    "# #        You do _not_ need select any scopes, i.e. leave all the checkboxes unchecked\n",
    "# #     2. Save it in your env.py file under the variable `github_token`\n",
    "# # TODO: Add your github username to your env.py file under the variable `github_username`\n",
    "# # TODO: Add more repositories to the `REPOS` list below.\n",
    "\n",
    "# REPOS = [\n",
    "#     \"gocodeup/codeup-setup-script\",\n",
    "#     \"gocodeup/movies-application\",\n",
    "#     \"torvalds/linux\", 'jtleek/datasharing',\n",
    "#     'rdpeng/ProgrammingAssignment2',\n",
    "#     'octocat/Spoon-Knife',\n",
    "#     'SmartThingsCommunity/SmartThingsPublic',\n",
    "#     'tensorflow/tensorflow',\n",
    "#     'twbs/bootstrap',\n",
    "#     'LSPosed/MagiskOnWSA',\n",
    "#     'github/gitignore',\n",
    "#     'Pierian-Data/Complete-Python-3-Bootcamp',\n",
    "#     'nightscout/cgm-remote-monitor',\n",
    "#     'jwasham/coding-interview-university',\n",
    "#     'rdpeng/ExData_Plotting1',\n",
    "#     'github/docs',\n",
    "#     'opencv/opencv',\n",
    "#     'EbookFoundation/free-programming-books',\n",
    "#     'eugenp/tutorials',\n",
    "#     'CyC2018/CS-Notes',\n",
    "#     'tensorflow/models',\n",
    "#     'jackfrued/Python-100-Days',\n",
    "#     'firstcontributions/first-contributions',\n",
    "#     'hitherejoe/BottomNavigationViewSample',\n",
    "#     'J-Rios/TLG_JoinCaptchaBot',\n",
    "#     'NeKosmic/NK-BOT',\n",
    "#     'r7kamura/ruboty',\n",
    "#     'MMMzq/bot_toast',\n",
    "#     'SpEcHiDe/PyroGramBot',\n",
    "#     'natario1/BottomSheetCoordinatorLayout',\n",
    "#     'BilalShahid13/PersistentBottomNavBar',\n",
    "#     'brucevanfdm/BottomNavigationView',\n",
    "#     'jaisonfdo/BottomNavigation',\n",
    "#     'germanattanasio/text-bot',\n",
    "#     'hikaruAi/FacebookBot',\n",
    "#     'xTCry/VCoin',\n",
    "#     'NNTin/discord-twitter-bot',\n",
    "#     'CharmingDays/kurusaki_voice',\n",
    "#     'CrazyBotsz/Adv-Auto-Filter-Bot',\n",
    "#     'paco0x/amm-arbitrageur',\n",
    "#     'nysamnang/react-native-raw-bottom-sheet',\n",
    "#     'fabston/TradingView-Webhook-Bot',\n",
    "#     '546669204/RebateBot',\n",
    "#     'topkecleon/telegram-bot-bash',\n",
    "#     'evil-mad/EggBot',\n",
    "#     'telegram-bot-rb/telegram-bot',\n",
    "#     'alfficcadenti/splinterlands-bot',\n",
    "#     'nodeWechat/wechat4u',\n",
    "#     'tucnak/telebot',\n",
    "#     'CarlGroth/Carl-Bot',\n",
    "#     'grapeot/WechatForwardBot',\n",
    "#     'googleworkspace/hangouts-chat-samples',\n",
    "#     'GreyWolfDev/Werewolf',\n",
    "#     'abdelhai/awesome-bots',\n",
    "#     'mdgspace/bot',\n",
    "#     'Sank6/Discord-Bot-List',\n",
    "#     'samc621/SneakerBot',\n",
    "#     'boto/boto3-sample',\n",
    "#     'huseinzol05/Stock-Prediction-Models',\n",
    "#     'SAPConversationalAI/Webchat',\n",
    "#     'soumyadityac/youtube-viewer',\n",
    "#     'CodeWithJoe2020/pancakeswapBot',\n",
    "#     'Merubokkusu/discord-spam-bots',\n",
    "#     'ZeroDiscord/EconomyBot',\n",
    "#     'scrapinghub/slackbot',\n",
    "#     'baidu/boteye',\n",
    "#     'kcloze/swoole-bot',\n",
    "#     'hyperchessbot/hyperbot',\n",
    "#     'yangyuan/hearthrock',\n",
    "#     'agermanidis/SnapchatBot',\n",
    "#     'brompwnie/botb',\n",
    "#     'thedevs-network/the-guard-bot',\n",
    "#     'jqs7/Jqs7Bot',\n",
    "#     'AdeelMufti/CryptoBot',\n",
    "#     'aracred/bot',\n",
    "#     'zeldaret/botw',\n",
    "#     'jh0ker/mau_mau_bot',\n",
    "#     'coq/bot',\n",
    "#     'sandimetz/1st_99bottles_ruby',\n",
    "#     'jsdelivr/bot',\n",
    "#     'mgp25/Telegram-Bot-API',\n",
    "#     'kereh/BOT',\n",
    "#     'torvalds/linux',\n",
    "#     'Snailclimb/JavaGuide',\n",
    "#     'facebook/react',\n",
    "#     'rdpeng/RepData_PeerAssessment1',\n",
    "#     'spring-projects/spring-boot',\n",
    "#     'jlord/patchwork',\n",
    "#     'TheAlgorithms/Python',\n",
    "#     'ant-design/ant-design',\n",
    "#     'barryclark/jekyll-now',\n",
    "#     'spring-projects/spring-framework',\n",
    "#     'kubernetes/kubernetes',\n",
    "#     'bitcoin/bitcoin',\n",
    "#     'vuejs/vue',\n",
    "#     'mrdoob/three.js',\n",
    "#     'DataScienceSpecialization/courses',\n",
    "#     'getify/You-Dont-Know-JS',\n",
    "#     'freeCodeCamp/freeCodeCamp',\n",
    "#     'angular/angular.js',\n",
    "#     'kamranahmedse/developer-roadmap',\n",
    "#     'PanJiaChen/vue-element-admin',\n",
    "#     'sindresorhus/awesome',\n",
    "#     'ohmyzsh/ohmyzsh',\n",
    "#     'trekhleb/javascript-algorithms',\n",
    "#     'flutter/flutter',\n",
    "#     'TheAlgorithms/Python',\n",
    "#     'chartjs/Chart.js',\n",
    "#     'coder/code-server',\n",
    "#     'nestjs/nest',\n",
    "#     'yangshun/tech-interview-handbook',\n",
    "#     'laravel/laravel',\n",
    "#     'trimstray/the-book-of-secret-knowledge',\n",
    "#     'ryanmcdermott/clean-code-javascript',\n",
    "#     'gothinkster/realworld',\n",
    "#     'vuejs/awesome-vue',\n",
    "#     'tonsky/FiraCode',\n",
    "#     'hakimel/reveal.js',\n",
    "#     'angular/angular.js',\n",
    "#     'reduxjs/redux',\n",
    "#     'tailwindlabs/tailwindcss',\n",
    "#     'ripienaar/free-for-dev',\n",
    "#     'protocolbuffers/protobuf',\n",
    "#     'shadowsocks/shadowsocks-windows',\n",
    "#     'JetBrains/kotlin',\n",
    "#     'yarnpkg/yarn',\n",
    "#     'TryGhost/Ghost',\n",
    "#     'square/retrofit',\n",
    "#     'bradtraversy/design-resources-for-developers',\n",
    "#     'vsouza/awesome-ios',\n",
    "#     'iamkun/dayjs',\n",
    "#     'google/googletest',\n",
    "#     'projectdiscovery/nuclei-templates',\n",
    "#     'digitalocean/nginxconfig.io',\n",
    "#     'flutter/flutter',\n",
    "#     'PaddlePaddle/PaddleOCR',\n",
    "#     'flutter/pinball',\n",
    "#     'wolfogre/go-pprof-practice',\n",
    "#     'supabase/supabase',\n",
    "#     'felipefialho/frontend-challenges',\n",
    "#     'flutter/samples',\n",
    "#     'alibaba/fastjson2',\n",
    "#     'florinpop17/app-ideas',\n",
    "#     'charmbracelet/bubbletea',\n",
    "#     'huggingface/transformers',\n",
    "#     'databricks-academy/data-engineering-with-databricks',\n",
    "#     'hectorqin/reader',\n",
    "#     'Azure/azure-rest-api-specs',\n",
    "#     'terra-money/core',\n",
    "#     'saltstack/salt',\n",
    "#     'PKUFlyingPig/cs-self-learning',\n",
    "#     'actions/virtual-environments',\n",
    "#     'jojoldu/junior-recruit-scheduler',\n",
    "#     'dotnet/aspnetcore',\n",
    "#     'danielgindi/Charts',\n",
    "#     'microsoft/unilm',\n",
    "#     'jtleek/datasharing',\n",
    "#     'rdpeng/ProgrammingAssignment2',\n",
    "#     'octocat/Spoon-Knife',\n",
    "#     'tensorflow/tensorflow',\n",
    "#     'twbs/bootstrap',\n",
    "#     'Pierian-Data/Complete-Python-3-Bootcamp',\n",
    "#     'nightscout/cgm-remote-monitor',\n",
    "#     'jwasham/coding-interview-university',\n",
    "#     'rdpeng/ExData_Plotting1',\n",
    "#     'github/docs',\n",
    "#     'github/docs',\n",
    "#     'opencv/opencv',\n",
    "#     'EbookFoundation/free-programming-books',\n",
    "#     'eugenp/tutorials',\n",
    "#     'CyC2018/CS-Notes',\n",
    "#     'jackfrued/Python-100-Days',\n",
    "#     'firstcontributions/first-contributions',\n",
    "#     'torvalds/linux',\n",
    "#     'Snailclimb/JavaGuide',\n",
    "#     'facebook/react',\n",
    "#     'jlord/patchwork',\n",
    "#     'TheAlgorithms/Python',\n",
    "#     'ant-design/ant-design',\n",
    "#     'barryclark/jekyll-now',\n",
    "#     'bitcoin/bitcoin',\n",
    "#     'angular/angular.js',\n",
    "#     'kamranahmedse/developer-roadmap',\n",
    "#     'PanJiaChen/vue-element-admin',\n",
    "#     'django/django',\n",
    "#     'mui/material-ui',\n",
    "#     'kamranahmedse/developer-roadmap',\n",
    "#     'PanJiaChen/vue-element-admin',\n",
    "#     'DefinitelyTyped/DefinitelyTyped',\n",
    "#     'django/django',\n",
    "#     'mui/material-ui',\n",
    "#     'RedHatTraining/DO180-apps',\n",
    "#     'qmk/qmk_firmware',\n",
    "#     'apache/spark',\n",
    "#     'apache/dubbo',\n",
    "#     'google/it-cert-automation-practice',\n",
    "#     'facebook/create-react-app',\n",
    "#     'airbnb/javascript',\n",
    "#     'git/git',\n",
    "#     'nodejs/node',\n",
    "#     'sindresorhus/awesome',\n",
    "#     'iluwatar/java-design-patterns',\n",
    "#     'python/cpython',\n",
    "#     'd3/d3',\n",
    "#     'scikit-learn/scikit-learn',\n",
    "#     'atralice/Curso.Prep.Henry',\n",
    "#     'OCA/sale-workflow',\n",
    "#     'forcedotcom/SalesforceMobileSDK-Android',\n",
    "#     'appleseedhq/appleseed',\n",
    "#     'jasondavies/d3-cloud',\n",
    "#     'phpDocumentor/phpDocumentor',\n",
    "#     'UZ-SLAMLab/ORB_SLAM3',\n",
    "#     'ExtendRealityLtd/VRTK',\n",
    "#     'gboeing/osmnx',\n",
    "#     'luvit/luvit',\n",
    "#     'jaredpalmer/razzle',\n",
    "#     'howdyai/botkit',\n",
    "#     'json-iterator/go',\n",
    "#     'docker/labs',\n",
    "#     'ionic-team/stencil',\n",
    "#     'charmbracelet/bubbletea',\n",
    "#     'jxnblk/mdx-deck',\n",
    "#     'rovo89/Xposed',\n",
    "#     'rauchg/slackin',\n",
    "#     'axi0mX/ipwndfu',\n",
    "#     'rshipp/awesome-malware-analysis',\n",
    "#     'squeaky-pl/japronto',\n",
    "#     'claudiodangelis/qrcp',\n",
    "#     'codeguy/php-the-right-way',\n",
    "#     'aristocratos/bpytop',\n",
    "#     'php-fig/fig-standards',\n",
    "#     'pyecharts/pyecharts',\n",
    "#     'arangodb/arangodb',\n",
    "#     'qmk/qmk_firmware',\n",
    "#     'MaterialDesignInXAML/MaterialDesignInXamlToolkit',\n",
    "#     'kubernetes-sigs/kubespray',\n",
    "#     'mgonto/restangular',\n",
    "#     'draveness/analyze',\n",
    "#     'danialfarid/ng-file-upload',\n",
    "#     'mathiasbynens/jquery-placeholder',\n",
    "#     'makovkastar/FloatingActionButton',\n",
    "#     'BVLC/caffe',\n",
    "#     'apache/echarts',\n",
    "#     'CSSEGISandData/COVID-19',\n",
    "#     'jenkins-docs/simple-java-maven-app',\n",
    "#     'vercel/next.js',\n",
    "#     'home-assistant/core',\n",
    "#     'moby/moby',\n",
    "#     'ColorlibHQ/AdminLTE',\n",
    "#     'scm-ninja/starter-web',\n",
    "#     'xingshaocheng/architect-awesome',\n",
    "#     'ArduPilot/ardupilot',\n",
    "#     'codebasics/py',\n",
    "#     'ageitgey/face_recognition',\n",
    "#     'bailicangdu/vue2-elm',\n",
    "#     'angular/angular-cli',\n",
    "#     'kdn251/interviews',\n",
    "#     'Trinea/android-open-project',\n",
    "#     'zero-to-mastery/start-here-guidelines',\n",
    "#     'FortAwesome/Font-Awesome',\n",
    "#     'Homebrew/legacy-homebrew',\n",
    "#     'jakevdp/PythonDataScienceHandbook',\n",
    "#     'aymericdamien/TensorFlow-Examples',\n",
    "#     'pallets/flask',\n",
    "#     'github/opensource.guide',\n",
    "#     'golang/go',\n",
    "#     'TheOdinProject/css-exercises',\n",
    "#     'selfteaching/the-craft-of-selfteaching',\n",
    "#     'pandas-dev/pandas',\n",
    "#     'ElemeFE/element',\n",
    "#     'ionic-team/ionic-framework',\n",
    "#     'doocs/advanced-java',\n",
    "#     'MarlinFirmware/Marlin',\n",
    "#     'shadowsocks/shadowsocks-windows',\n",
    "#     'CoreyMSchafer/code_snippets',\n",
    "#     'MicrosoftDocs/azure-docs',\n",
    "#     'odoo/odoo',\n",
    "#     'FreeRDP/FreeRDP',\n",
    "#     'hakimel/reveal.js',\n",
    "#     'gabrielecirulli/2048',\n",
    "#     'udacity/course-collaboration-travel-plans',\n",
    "#     'ossu/computer-science',\n",
    "#     'jakevdp/PythonDataScienceHandbook',\n",
    "#     'aymericdamien/TensorFlow-Examples',\n",
    "#     'reduxjs/redux',\n",
    "#     'pallets/flask',\n",
    "#     'TheAlgorithms/Java',\n",
    "#     'scutan90/DeepLearning-500-questions',\n",
    "#     'huggingface/transformers',\n",
    "#     'github/opensource.guide',\n",
    "#     'golang/go',\n",
    "#     'reduxjs/redux',\n",
    "#     'pallets/flask',\n",
    "#     'scutan90/DeepLearning-500-questions',\n",
    "#     'huggingface/transformers',\n",
    "#     'github/opensource.guide',\n",
    "#     'golang/go',\n",
    "#     'TheOdinProject/css-exercises',\n",
    "#     'selfteaching/the-craft-of-selfteaching',\n",
    "#     'netty/netty',\n",
    "#     'Azure/azure-quickstart-templates'\n",
    "# ]\n",
    "\n",
    "# headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "# if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "#     raise Exception(\n",
    "#         \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def github_api_request(url: str) -> Union[List, Dict]:\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     response_data = response.json()\n",
    "#     if response.status_code != 200:\n",
    "#         print(f'error making request {url}')\n",
    "#         raise Exception(\n",
    "#             f\"Error response from github api! status code: {response.status_code}, \"\n",
    "#             f\"response: {json.dumps(response_data)}\"\n",
    "#         )\n",
    "#     return response_data\n",
    "\n",
    "\n",
    "# def get_repo_language(repo: str) -> str:\n",
    "#     url = f\"https://api.github.com/repos/{repo}\"\n",
    "#     repo_info = github_api_request(url)\n",
    "#     if type(repo_info) is dict:\n",
    "#         repo_info = cast(Dict, repo_info)\n",
    "#         if \"language\" not in repo_info:\n",
    "#             raise Exception(\n",
    "#                 \"'language' key not round in response\\n{}\".format(json.dumps(repo_info))\n",
    "#             )\n",
    "#         return repo_info[\"language\"]\n",
    "#     raise Exception(\n",
    "#         f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "#     url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "#     contents = github_api_request(url)\n",
    "#     if type(contents) is list:\n",
    "#         contents = cast(List, contents)\n",
    "#         return contents\n",
    "#     raise Exception(\n",
    "#         f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "#     \"\"\"\n",
    "#     Takes in a response from the github api that lists the files in a repo and\n",
    "#     returns the url that can be used to download the repo's README file.\n",
    "#     \"\"\"\n",
    "#     for file in files:\n",
    "#         if file[\"name\"].lower().startswith(\"readme\"):\n",
    "#             return file[\"download_url\"]\n",
    "#     return \"\"\n",
    "\n",
    "\n",
    "# def process_repo(repo: str) -> Dict[str, str]:\n",
    "#     \"\"\"\n",
    "#     Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "#     dictionary with the language of the repo and the readme contents.\n",
    "#     \"\"\"\n",
    "#     contents = get_repo_contents(repo)\n",
    "#     readme_download_url = get_readme_download_url(contents)\n",
    "#     if readme_download_url == \"\":\n",
    "#         readme_contents = \"\"\n",
    "#     else:\n",
    "#         readme_contents = requests.get(readme_download_url).text\n",
    "#     return {\n",
    "#         \"repo\": repo,\n",
    "#         \"language\": get_repo_language(repo),\n",
    "#         \"readme_contents\": readme_contents,\n",
    "#     }\n",
    "\n",
    "\n",
    "# def scrape_github_data() -> List[Dict[str, str]]:\n",
    "#     \"\"\"\n",
    "#     Loop through all of the repos and process them. Returns the processed data.\n",
    "#     \"\"\"\n",
    "#     return [process_repo(repo) for repo in REPOS]\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     data = scrape_github_data()\n",
    "#     json.dump(data, open(\"data.json\", \"w\"), indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9a7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import os\n",
    "import acquire as a\n",
    "import matt_prepare as p\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set default style for charts\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('fivethirtyeight')\n",
    "# change jupyter notebook setting to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d211aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a0ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11fab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9833c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61a4a4",
   "metadata": {},
   "source": [
    "### Prepare the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9552819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def basic_clean(string):\n",
    "#     '''\n",
    "#     This function takes in a string and\n",
    "#     returns the string normalized.\n",
    "#     '''\n",
    "#     string = unicodedata.normalize('NFKD', string)\\\n",
    "#             .encode('ascii', 'ignore')\\\n",
    "#             .decode('utf-8', 'ignore')\n",
    "#     string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "#     return string\n",
    "\n",
    "# def tokenize(string):\n",
    "#     '''\n",
    "#     This function takes in a string and\n",
    "#     returns a tokenized string.\n",
    "#     '''\n",
    "#     # Create tokenizer.\n",
    "#     tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "#     # Use tokenizer\n",
    "#     string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "#     return string\n",
    "\n",
    "# def stem(string):\n",
    "#     '''\n",
    "#     This function takes in a string and\n",
    "#     returns a string with words stemmed.\n",
    "#     '''\n",
    "#     # Create porter stemmer.\n",
    "#     ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "#     # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "#     stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "#     # Join our lists of words into a string again and assign to a variable.\n",
    "#     string = ' '.join(stems)\n",
    "    \n",
    "#     return string\n",
    "\n",
    "# def lemmatize(string):\n",
    "#     '''\n",
    "#     This function takes in string for and\n",
    "#     returns a string with words lemmatized.\n",
    "#     '''\n",
    "#     # Create the lemmatizer.\n",
    "#     wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "#     # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "#     lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "#     # Join our list of words into a string again and assign to a variable.\n",
    "#     string = ' '.join(lemmas)\n",
    "    \n",
    "#     return string\n",
    "\n",
    "# def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "#     '''\n",
    "#     This function takes in a string, optional extra_words and exclude_words parameters\n",
    "#     with default empty lists and returns a string.\n",
    "#     '''\n",
    "#     # Create stopword_list.\n",
    "#     stopword_list = stopwords.words('english')\n",
    "    \n",
    "#     # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "#     stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "#     # Add in 'extra_words' to stopword_list.\n",
    "#     stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "#     # Split words in string.\n",
    "#     words = string.split()\n",
    "    \n",
    "#     # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "#     filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "#     # Join words in the list back into strings and assign to a variable.\n",
    "#     string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7663445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['clean'] = df.readme_contents.apply(basic_clean)\n",
    "# tokenized_df = df.clean.apply(tokenize)\n",
    "# df['stemmed'] = tokenized_df.apply(stem)\n",
    "# df['lemmatized'] = tokenized_df.apply(lemmatize)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a934200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create function for fully prepared data...\n",
    "\n",
    "# def prep_readme_df(extra_words=[], exclude_words=[]):\n",
    "#     '''\n",
    "#     This function take in a df and the string name for a text column with \n",
    "#     option to pass lists for extra_words and exclude_words and\n",
    "#     returns a df with the text article title, original text, stemmed text,\n",
    "#     lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "#     '''\n",
    "\n",
    "#     df = acquire.scrape_github_data()\n",
    "\n",
    "#     df['clean'] = df['readme_contents'].astype(str).apply(basic_clean)\\\n",
    "#                             .apply(tokenize)\\\n",
    "#                             .apply(remove_stopwords, \n",
    "#                              extra_words=extra_words, \n",
    "#                              exclude_words=exclude_words)  \n",
    "#     df['stemmed'] = df['clean'].apply(stem)\n",
    "#     df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "\n",
    "#     # Character and word counts\n",
    "#     df = df.assign(character_count= df.lemmatized.str.len(), \n",
    "#                    word_count=df.lemmatized.str.split().apply(len))\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7d00aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gocodeup/codeup-setup-script</td>\n",
       "      <td>Shell</td>\n",
       "      <td># Codeup Setup Script\\n\\nSetup script for Code...</td>\n",
       "      <td>codeup setup script\\n\\nsetup script for codeu...</td>\n",
       "      <td>codeup setup script setup script for codeup st...</td>\n",
       "      <td>codeup setup script setup script for codeup st...</td>\n",
       "      <td>1395</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gocodeup/movies-application</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Movies Application\\n\\nFor this project, we w...</td>\n",
       "      <td>movies application\\n\\nfor this project we wil...</td>\n",
       "      <td>movi applic for thi project we will be build a...</td>\n",
       "      <td>movie application for this project we will be ...</td>\n",
       "      <td>4291</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torvalds/linux</td>\n",
       "      <td>C</td>\n",
       "      <td>Linux kernel\\n============\\n\\nThere are severa...</td>\n",
       "      <td>linux kernel\\n\\n\\nthere are several guides for...</td>\n",
       "      <td>linux kernel there are sever guid for kernel d...</td>\n",
       "      <td>linux kernel there are several guide for kerne...</td>\n",
       "      <td>657</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jtleek/datasharing</td>\n",
       "      <td>None</td>\n",
       "      <td>How to share data with a statistician\\n=======...</td>\n",
       "      <td>how to share data with a statistician\\n\\n\\nthi...</td>\n",
       "      <td>how to share data with a statistician thi is a...</td>\n",
       "      <td>how to share data with a statistician this is ...</td>\n",
       "      <td>12154</td>\n",
       "      <td>2030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdpeng/ProgrammingAssignment2</td>\n",
       "      <td>R</td>\n",
       "      <td>### Introduction\\n\\nThis second programming as...</td>\n",
       "      <td>introduction\\n\\nthis second programming assig...</td>\n",
       "      <td>introduct thi second program assign will requi...</td>\n",
       "      <td>introduction this second programming assignmen...</td>\n",
       "      <td>3554</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            repo    language  \\\n",
       "0   gocodeup/codeup-setup-script       Shell   \n",
       "1    gocodeup/movies-application  JavaScript   \n",
       "2                 torvalds/linux           C   \n",
       "3             jtleek/datasharing        None   \n",
       "4  rdpeng/ProgrammingAssignment2           R   \n",
       "\n",
       "                                     readme_contents  \\\n",
       "0  # Codeup Setup Script\\n\\nSetup script for Code...   \n",
       "1  # Movies Application\\n\\nFor this project, we w...   \n",
       "2  Linux kernel\\n============\\n\\nThere are severa...   \n",
       "3  How to share data with a statistician\\n=======...   \n",
       "4  ### Introduction\\n\\nThis second programming as...   \n",
       "\n",
       "                                               clean  \\\n",
       "0   codeup setup script\\n\\nsetup script for codeu...   \n",
       "1   movies application\\n\\nfor this project we wil...   \n",
       "2  linux kernel\\n\\n\\nthere are several guides for...   \n",
       "3  how to share data with a statistician\\n\\n\\nthi...   \n",
       "4   introduction\\n\\nthis second programming assig...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  codeup setup script setup script for codeup st...   \n",
       "1  movi applic for thi project we will be build a...   \n",
       "2  linux kernel there are sever guid for kernel d...   \n",
       "3  how to share data with a statistician thi is a...   \n",
       "4  introduct thi second program assign will requi...   \n",
       "\n",
       "                                          lemmatized  character_count  \\\n",
       "0  codeup setup script setup script for codeup st...             1395   \n",
       "1  movie application for this project we will be ...             4291   \n",
       "2  linux kernel there are several guide for kerne...              657   \n",
       "3  how to share data with a statistician this is ...            12154   \n",
       "4  introduction this second programming assignmen...             3554   \n",
       "\n",
       "   word_count  \n",
       "0         203  \n",
       "1         768  \n",
       "2          97  \n",
       "3        2030  \n",
       "4         620  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = p.clean_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c64d2",
   "metadata": {},
   "source": [
    "### Split the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2431284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(df):\n",
    "#     '''\n",
    "#     This function takes in a data frame and splits it appropriately in order\n",
    "#     to return a train with 56%, validate with 24%, and test with 20% of the\n",
    "#     original data frame.\n",
    "#     '''\n",
    "#     # Split with train being 80% and test being 20%. Stratify on target.\n",
    "#     train, test = train_test_split(df, test_size = .2, random_state = 123, stratify = df.language)\n",
    "#     # Split the remaining train into 70% train and 30% validate.\n",
    "#     train, validate = train_test_split(train, test_size = .3, random_state = 123)\n",
    "#     # Spiltting results in a split with 56% train, 24% validate, and 20% test data from original\n",
    "#     return train, validate, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2604c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x6/9q2vjsz90nx_0lgx5gr8g33w0000gn/T/ipykernel_12533/665729508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/team2-nlp-project/matt_prepare.py\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    140\u001b[0m     '''\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Split with train being 80% and test being 20%. Stratify on target.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Split the remaining train into 70% train and 30% validate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "train, validate, test = p.split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e499255",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.language.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.language.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da962073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
