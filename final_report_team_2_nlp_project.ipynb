{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e5a551",
   "metadata": {},
   "source": [
    "# Final Report - Team 2 NLP Project\n",
    "\n",
    "Ella Xu, Jerry Nolf, Matthew Luna, Nathan Sharick - Innis Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737079c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631cfc0",
   "metadata": {},
   "source": [
    "### Project Description:\n",
    "\n",
    "- Most of the code hosting platforms for opensource projects consider the README file as the project introduction. As it is the first document seen by the reader, such a document needs to be crafted with care. The goal of this project is to predict the programming language for 100 repository by scraping, analyze the repository's README file contents. Using these datasets from 100 README's we were able to predict what programming language was used based on the composition of the README text.\n",
    "\n",
    "### Project Goal:\n",
    "\n",
    "- The goal of this project was to build a classification model that can predict the programming language of a repository based on the text of the repository's README.md file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb2e58",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71936ea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (visualization.py, line 15)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/jerrynolf/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3444\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/x6/9q2vjsz90nx_0lgx5gr8g33w0000gn/T/ipykernel_4314/1059333881.py\"\u001b[0;36m, line \u001b[0;32m13\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import visualization\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/jerrynolf/codeup-data-science/team2-nlp-project/visualization.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    \"\"\"This function is to create a visual to show and get top five languages. \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matt_prepare\n",
    "import visualization\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0f55b",
   "metadata": {},
   "source": [
    "### Data Acquisition, Data Cleaning, and Data Preparation\n",
    "\n",
    "- Web scraping methods were used to create a list of github username/repositories that included more than 300 repositories. The repositoriy names were pulled from github's top trending repositories, most forked repositories, and most stared repositories as of 05/13/ 2022.\n",
    "\n",
    "- The list of repositories was put into the acompanying acquire.py file which creates a list of dictionaries that includes the name of the repository, the programming language used in the repository, and the content of the readme file for each repository in the list, and saves it as a .json file. The .json file is required to reproduce this project with this notebook and can be created by saving the acquire.py file in your local repository and running 'python acquire.py' from the terminal.\n",
    "\n",
    "- Once the .json file is saved in the local directory is can be puled into the notebook, cleaned and prepared using the clean_df function located in the matt_prepare.py file. This function performs the following cleaning/preparation actions:\n",
    "    - It uses pandas to read the file into the notebook\n",
    "    \n",
    "    - It cleans the data by normalizing it, changing all words to lower case, and removing any characters that are not letters, numbers, or whitespace\n",
    "    \n",
    "    - It tokenizes the words in the readme content using a toktok tokenizer\n",
    "    \n",
    "    - It removes standard english stopwords from the readme content\n",
    "    \n",
    "    - It then outputs a dataframe with the following columns:\n",
    "        - name of the repository\n",
    "        \n",
    "        - Programming language of the repository\n",
    "        \n",
    "        - The raw readme contents\n",
    "        \n",
    "        - The cleaned readme content\n",
    "        \n",
    "        - The cleaned readme content that has been stemmed using a PorterStemmer\n",
    "        \n",
    "        - The cleaned readme content that has been lemmatized using a WordNetLemmatizer\n",
    "        \n",
    "        - The character count for the readme content\n",
    "        \n",
    "        - The word count for the readme content\n",
    "        \n",
    "- The dataset is then split into train, validate, and test sets using the split_data function located in the matt_prepare.py file which returns a train set with 56%, validate set with 24%, and test set with 20% of the original data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in, clean, and prepare the dataset using the clean_df function\n",
    "df = matt_prepare.clean_df()\n",
    "#view the first five rows of the returned dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train, validate, and test using the split_data function\n",
    "train, validate, test = matt_prepare.split_data(df)\n",
    "#view the row and column counts of the split dataframes\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd2495",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb576c",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "- Data exploration comments....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c69169",
   "metadata": {},
   "source": [
    "**Question 1:** What are the most common languages from the repos we explored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be019eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.top_languages(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0758f5",
   "metadata": {},
   "source": [
    "***Takeaways: Javascript and Python are two most frequently used languages.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80742adf",
   "metadata": {},
   "source": [
    "**Question 2:** What are the most common words across TOP languages README?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074dac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.word_cloud(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724be74",
   "metadata": {},
   "source": [
    "***Takeaways: There are popular words across top languages readme such as aligncenter, application, build, web, p....***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d53fd",
   "metadata": {},
   "source": [
    "**Question 3:** Are there any relationship between char count and word count? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.char_word(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb0ace",
   "metadata": {},
   "source": [
    "#### Statistical Testing (Correlation):\n",
    "\n",
    "\n",
    "#### Hypothesis:\n",
    "\n",
    " - H0: There is no linear relationship between character count and word count of a READme.\n",
    "\n",
    " - Ha: There is a linear relationship between character count and word count of a READme.\n",
    "\n",
    "A Pearson's r statistical test will allow us to verify our beliefs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.question3_stats(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bfda76",
   "metadata": {},
   "source": [
    "***Takeaways: There is a positive relationship between character count vs word count.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a8f50",
   "metadata": {},
   "source": [
    "**Question 4:** Does the length of READme differ between languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.char_count(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f944946",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.question4_stats(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed7f1d",
   "metadata": {},
   "source": [
    "***Takeaways: By Average, JavaScript has the longest character count across all languages, follows by TypeScript***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.word_count(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e2580",
   "metadata": {},
   "source": [
    "***Takeaways: JavaScript has has the longest word count across all languages, follows by other and they python, compare word and character count,python has longer word count, but Typescript has longer character count.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc69df0",
   "metadata": {},
   "source": [
    "### Takeaways from Data Exploration\n",
    "\n",
    "- Takeaway notes......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18ae84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f14607",
   "metadata": {},
   "source": [
    "### Data Preparation for Modeling\n",
    "\n",
    "- Before proceding with modeling the data from each of the split groups was assigned to variable that can be used with the vectorizers and the classification models during the modeling phase\n",
    "\n",
    "- The cleaned and lemmatized readme contents from each of the split groups was assigned to an X variable for the group\n",
    "\n",
    "- The engineered language classification column (all language identifers were either one of the top five from the dataset or assigned as 'other') from each split dataset was assigned to a y variable for the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the cleaned and lemmatized readme contents from the train set to the X_train variable\n",
    "X_train = train.lemmatized\n",
    "#assign the engineered language classification value from the train set to the y_train variable\n",
    "y_train = train.top_five_languages\n",
    "#assign the cleaned and lemmatized readme contents from the validate set to the X_val variable\n",
    "X_val = validate.lemmatized\n",
    "#assign the engineered language classification value from the validate set to the y_val variable\n",
    "y_val = validate.top_five_languages\n",
    "#assign the cleaned and lemmatized readme contents from the test set to the X_test variable\n",
    "X_test = test.lemmatized\n",
    "#assign the engineered language classification value from the test set to the y_test variable\n",
    "y_test = test.top_five_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4691c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14601168",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "- Two types of vectorizers (CountVectorizer and TfidfVectorizer) were used to preprocess the data prior to running the classification modeling\n",
    "\n",
    "- Over 100 models were developed using both sets of vectorized data, multiple classification model types (decision tree, logistic regression, random forest, etc) and a full range of hyperperameters for each model type were evaluated.\n",
    "\n",
    "- The top eight models with their optimal hyperparameters and vectorized datasets are seen below\n",
    "\n",
    "- Each model's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##modeling with unigrams## \n",
    "\n",
    "#create the CountVectorizer object\n",
    "cv = CountVectorizer()\n",
    "#fit the CountVectorizer with the train data and transform the train data\n",
    "X_bow = cv.fit_transform(X_train)\n",
    "#transform the validate data with the CountVectorizer\n",
    "X_bow_val = cv.transform(X_val)\n",
    "\n",
    "#create the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "#fit the TfidfVectorizer with the train data and transform the train data\n",
    "X_tfidf = tfidf.fit_transform(X_train)\n",
    "#transform the validate data\n",
    "X_tfidf_val = tfidf.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the decision tree classifier object with a max depth of 15 and a random state of 123 for reproducibility\n",
    "tree = DecisionTreeClassifier(max_depth=15, random_state=123)\n",
    "#fit the decision tree with the CountVectorizer train data\n",
    "tree.fit(X_bow, y_train)\n",
    "#calculate the accuracy score of the decision tree with countvectorizer train and validate data\n",
    "tree.score(X_bow, y_train), tree.score(X_bow_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88292966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the decision tree classifier object with a max depth of 15 and a random state of 123 for reproducibility\n",
    "tree = DecisionTreeClassifier(max_depth=15, random_state=123)\n",
    "#fit the decision tree with the tfidfvectorizer train data\n",
    "tree.fit(X_tfidf, y_train)\n",
    "#calculate the accuracy score of the decision tree with tfidfvectorizer train and validate data\n",
    "tree.score(X_tfidf, y_train), tree.score(X_tfidf_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the logistic regression classifier object with a C value of 0.05 and a random state of 123 for reproducability\n",
    "#and fit it with the countvectorizer train data\n",
    "lm = LogisticRegression(C=0.05, random_state=123).fit(X_bow, y_train)\n",
    "#calculate the accuracy score of the model with countvectorizer train and validate data\n",
    "lm.score(X_bow, y_train), lm.score(X_bow_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the logistic regression classifier object with a C value of 10 and a random state of 123 for reproducability\n",
    "#and fit it with the tfidfvectorizer train data\n",
    "lm = LogisticRegression(C=10, random_state=123).fit(X_tfidf, y_train)\n",
    "#calculate the accuracy score of the model with tfidfvectorizer train and validate data\n",
    "lm.score(X_tfidf, y_train), lm.score(X_tfidf_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf159ea4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling with unigrams and bigrams\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "X_bow = cv.fit_transform(X_train)\n",
    "X_bow_val = cv.transform(X_val)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(X_train)\n",
    "X_tfidf_val = tfidf.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=20, random_state=123)\n",
    "tree.fit(X_bow, y_train)\n",
    "tree.score(X_bow, y_train), tree.score(X_bow_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9bcd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, random_state=123)\n",
    "tree.fit(X_tfidf, y_train)\n",
    "tree.score(X_tfidf, y_train), tree.score(X_tfidf_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LogisticRegression(C=0.1, random_state=123).fit(X_bow, y_train)\n",
    "lm.score(X_bow, y_train), lm.score(X_bow_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85421a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LogisticRegression(C=500, random_state=123).fit(X_tfidf, y_train)\n",
    "lm.score(X_tfidf, y_train), lm.score(X_tfidf_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42103a0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c2bb0",
   "metadata": {},
   "source": [
    "**Best Model with the Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X_train)\n",
    "X_tfidf_val = tfidf.transform(X_val)\n",
    "X_tfidf_test = tfidf.transform(X_test)\n",
    "lm = LogisticRegression(C=10, random_state=123).fit(X_tfidf, y_train)\n",
    "lm.score(X_tfidf, y_train), lm.score(X_tfidf_val, y_val), lm.score(X_tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f019baa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best model with test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d8bd0",
   "metadata": {},
   "source": [
    "### Results from Modeling\n",
    "\n",
    "- Notes on modeling results ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda393e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05275bd",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Summary notes ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2bc4e",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "- Notes for next steps ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7eb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
